{"authors": [{"first": "Feng", "last": "Qian"}, {"first": "Bo", "last": "Han"}, {"first": "Qingyang", "last": "Xiao"}, {"first": "Vijay", "last": "Gopalakrishnan"}], "title": "Flare: Practical Viewport-Adaptive 360-Degree Video Streaming for Mobile Devices", "year": "2018", "isbn": "9781450359030", "publisher": "Association for Computing Machinery", "address": "New York, NY, USA", "url": "https://doi.org/10.1145/3241539.3241565", "doi": "10.1145/3241539.3241565", "abstract": "Flare is a practical system for streaming 360-degree videos on commodity mobile devices. It takes a viewport-adaptive approach, which fetches only portions of a panoramic scene that cover what a viewer is about to perceive. We conduct an IRB-approved user study where we collect head movement traces from 130 diverse users to gain insights on how to design the viewport prediction mechanism for Flare. We then develop novel online algorithms that determine which spatial portions to fetch and their corresponding qualities. We also innovate other components in the streaming pipeline such as decoding and server-side transmission. Through extensive evaluations (~400 hours' playback on WiFi and ~100 hours over LTE), we show that Flare significantly improves the QoE in real-world settings. Compared to non-viewport-adaptive approaches, Flare yields up to 18x quality level improvement on WiFi, and achieves high bandwidth reduction (up to 35%) and video quality enhancement (up to 4.9x) on LTE.", "booktitle": "Proceedings of the 24th Annual International Conference on Mobile Computing and Networking", "numpages": "16", "keywords": ["tiles", "viewport-adaptation", "smartphones", "panoramic videos", "video decoding", "virtual reality", "360-degree videos"], "location": "New Delhi, India", "series": "MobiCom '18"}