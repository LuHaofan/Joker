{"authors": [{"first": "Siddhant", "last": "Prakash"}, {"first": "Alireza", "last": ""}, {"first": "Linda D.", "last": "Nguyen"}, {"first": "Robert", "last": "LiKamWa"}], "title": "GLEAM: An Illumination Estimation Framework for Real-Time Photorealistic Augmented Reality on Mobile Devices", "year": "2019", "isbn": "9781450366618", "publisher": "Association for Computing Machinery", "address": "New York, NY, USA", "url": "https://doi.org/10.1145/3307334.3326098", "doi": "10.1145/3307334.3326098", "abstract": "Mixed reality mobile platforms attempt to co-locate virtual scenes with physical environments, towards creating immersive user experiences. However, to create visual harmony between virtual and physical spaces, the virtual scene must be accurately illuminated with realistic lighting that matches the physical environment. To this end, we design GLEAM, a framework that provides robust illumination estimation in real-time by integrating physical light-probe estimation with current mobile AR systems. GLEAM visually observes reflective objects to compose a realistic estimation of physical lighting. Optionally, GLEAM can network multiple devices to sense illumination from different viewpoints and compose a richer estimation to enhance realism and fidelity. Using GLEAM, AR developers gain the freedom to use a wide range of materials, which is currently limited by the unrealistic appearance of materials that need accurate illumination, such as liquids, glass, and smooth metals. Our controlled environment user studies across 30 participants reveal the effectiveness of GLEAM in providing robust and adaptive illumination estimation over commercial status quo solutions, such as pre-baked directional lighting and ARKit 2.0 illumination estimation. Our benchmarks reveal the need for situation driven tradeoffs to optimize for quality factors in situations requiring freshness over quality and vice-versa. Optimizing for different quality factors in different situations, GLEAM can update scene illumination as fast as 30ms by sacrificing richness and fidelity in highly dynamic scenes, or prioritize quality by allowing an update interval as high as 400ms in scenes that require high-fidelity estimation.", "booktitle": "Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services", "numpages": "13", "keywords": ["image processing", "image-based lighting", "light estimation", "light probe", "augmented reality", "geometry", "lighting models"], "location": "Seoul, Republic of Korea", "series": "MobiSys '19"}