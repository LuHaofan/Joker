{"authors": [{"first": "Shin", "last": "Katayama"}, {"first": "Akhil", "last": "Mathur"}, {"first": "Tadashi", "last": "Okoshi"}, {"first": "Jin", "last": "Nakazawa"}, {"first": "Fahim", "last": "Kawsar"}], "title": "Situation-Aware Conversational Agent with Kinetic Earables (Demo)", "year": "2019", "isbn": "9781450366618", "publisher": "Association for Computing Machinery", "address": "New York, NY, USA", "url": "https://doi.org/10.1145/3307334.3328569", "doi": "10.1145/3307334.3328569", "abstract": "Conversational agents are increasingly becoming digital partners of our everyday computing experiences offering a variety of purposeful information and utility services. Although rich on competency, these agents are entirely oblivious to their users' situational and emotional context today and incapable of adjusting their interaction style and tone contextually. To this end, we present a first-of-its-kind situation-aware conversational agent on kinetic earable that dynamically adjusts its conversation style, tone, volume in response to users emotional, environmental, social and activity context gathered through speech prosody, ambient sound and motion signatures.", "booktitle": "Proceedings of the 17th Annual International Conference on Mobile Systems, Applications, and Services", "numpages": "2", "keywords": ["emotion regulation", "earables", "conversational agent", "context awareness"], "location": "Seoul, Republic of Korea", "series": "MobiSys '19"}