{"authors": [{"first": "Wei", "last": "Wang"}, {"first": "Alex X.", "last": "Liu"}, {"first": "Ke", "last": "Sun"}], "title": "Device-Free Gesture Tracking Using Acoustic Signals", "year": "2016", "isbn": "9781450342261", "publisher": "Association for Computing Machinery", "address": "New York, NY, USA", "url": "https://doi.org/10.1145/2973750.2973764", "doi": "10.1145/2973750.2973764", "abstract": "Device-free gesture tracking is an enabling HCI mechanism for small wearable devices because fingers are too big to control the GUI elements on such small screens, and it is also an important HCI mechanism for medium-to-large size mobile devices because it allows users to provide input without blocking screen view. In this paper, we propose LLAP, a device-free gesture tracking scheme that can be deployed on existing mobile devices as software, without any hardware modification. We use speakers and microphones that already exist on most mobile devices to perform device-free tracking of a hand/finger. The key idea is to use acoustic phase to get fine-grained movement direction and movement distance measurements. LLAP first extracts the sound signal reflected by the moving hand/finger after removing the background sound signals that are relatively consistent over time. LLAP then measures the phase changes of the sound signals caused by hand/finger movements and then converts the phase changes into the distance of the movement. We implemented and evaluated LLAP using commercial-off-the-shelf mobile phones. For 1-D hand movement and 2-D drawing in the air, LLAP has a tracking accuracy of 3.5 mm and 4.6 mm, respectively. Using gesture traces tracked by LLAP, we can recognize the characters and short words drawn in the air with an accuracy of 92.3% and 91.2%, respectively.", "booktitle": "Proceedings of the 22nd Annual International Conference on Mobile Computing and Networking", "numpages": "13", "keywords": ["ultrasound", "device-free", "gesture tracking"], "location": "New York City, New York", "series": "MobiCom '16"}