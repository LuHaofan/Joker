{"authors": [{"first": "Li Lyna", "last": "Zhang"}, {"first": "Shihao", "last": "Han"}, {"first": "Jianyu", "last": "Wei"}, {"first": "Ningxin", "last": "Zheng"}, {"first": "Ting", "last": "Cao"}, {"first": "Yuqing", "last": "Yang"}, {"first": "Yunxin", "last": "Liu"}], "title": "Nn-Meter: Towards Accurate Latency Prediction of Deep-Learning Model Inference on Diverse Edge Devices", "year": "2021", "isbn": "9781450384438", "publisher": "Association for Computing Machinery", "address": "New York, NY, USA", "url": "https://doi.org/10.1145/3458864.3467882", "doi": "10.1145/3458864.3467882", "abstract": "With the recent trend of on-device deep learning, inference latency has become a crucial metric in running Deep Neural Network (DNN) models on various mobile and edge devices. To this end, latency prediction of DNN model inference is highly desirable for many tasks where measuring the latency on real devices is infeasible or too costly, such as searching for efficient DNN models with latency constraints from a huge model-design space. Yet it is very challenging and existing approaches fail to achieve a high accuracy of prediction, due to the varying model-inference latency caused by the runtime optimizations on diverse edge devices.In this paper, we propose and develop nn-Meter, a novel and efficient system to accurately predict the inference latency of DNN models on diverse edge devices. The key idea of nn-Meter is dividing a whole model inference into kernels, i.e., the execution units on a device, and conducting kernel-level prediction. nn-Meter builds atop two key techniques: (i) kernel detection to automatically detect the execution unit of model inference via a set of well-designed test cases; and (ii) adaptive sampling to efficiently sample the most beneficial configurations from a large space to build accurate kernel-level latency predictors. Implemented on three popular platforms of edge hardware (mobile CPU, mobile GPU, and Intel VPU) and evaluated using a large dataset of 26,000 models, nn-Meter significantly outperforms the prior state-of-the-art.", "booktitle": "Proceedings of the 19th Annual International Conference on Mobile Systems, Applications, and Services", "numpages": "13", "keywords": ["deep neural network", "edge AI", "inference latency prediction"], "location": "Virtual Event, Wisconsin", "series": "MobiSys '21"}