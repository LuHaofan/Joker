{"authors": [{"first": "Hao", "last": "Wu"}, {"first": "Jinghao", "last": "Feng"}, {"first": "Xuejin", "last": "Tian"}, {"first": "Edward", "last": "Sun"}, {"first": "Yunxin", "last": "Liu"}, {"first": "Bo", "last": "Dong"}, {"first": "Fengyuan", "last": "Xu"}, {"first": "Sheng", "last": "Zhong"}], "title": "EMO: Real-Time Emotion Recognition from Single-Eye Images for Resource-Constrained Eyewear Devices", "year": "2020", "isbn": "9781450379540", "publisher": "Association for Computing Machinery", "address": "New York, NY, USA", "url": "https://doi.org/10.1145/3386901.3388917", "doi": "10.1145/3386901.3388917", "abstract": "Real-time user emotion recognition is highly desirable for many applications on eyewear devices like smart glasses. However, it is very challenging to enable this capability on such devices due to tightly constrained image contents (only eye-area images available from the on-device eye-tracking camera) and computing resources of the embedded system. In this paper, we propose and develop a novel system called EMO that can recognize, on top of a resource-limited eyewear device, real-time emotions of the user who wears it. Unlike most existing solutions that require whole-face images to recognize emotions, EMO only utilizes the single-eye-area images captured by the eye-tracking camera of the eyewear. To achieve this, we design a customized deep-learning network to effectively extract emotional features from input single-eye images and a personalized feature classifier to accurately identify a user's emotions. EMO also exploits the temporal locality and feature similarity among consecutive video frames of the eye-tracking camera to further reduce the recognition latency and system resource usage. We implement EMO on two hardware platforms and conduct comprehensive experimental evaluations. Our results demonstrate that EMO can continuously recognize seven-type emotions at 12.8 frames per second with a mean accuracy of 72.2%, significantly outperforming the state-of-the-art approach, and consume much fewer system resources.", "booktitle": "Proceedings of the 18th International Conference on Mobile Systems, Applications, and Services", "numpages": "14", "keywords": ["deep learning", "eyewear devices", "emotion recognition", "single-eye images", "visual sensing"], "location": "Toronto, Ontario, Canada", "series": "MobiSys '20"}